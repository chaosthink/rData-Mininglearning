###第六部分，正则化 文本回归
###多项式回归能适用于非线性模型，但会产生相应的噪声

#产生正弦数据
set.seed(1)
x<- seq(0,1,by=0.01)
y<- sin(2*pi*x)+rnorm(length(x),0,0.1)
df<- data.frame(x=x,y=y)
ggplot(df,aes(x=x,y=y))+geom_point()+geom_smooth(method='lm',se=F)


##尝试拟合线性模型
summary(lm(y~x,data=df))
###效果还可以，但显然是存在问题的
df<- transform(df,x2=x^2)
df<- transform(df,x3=x^3) ##增加两列数据
summary(lm(y~x+x2+x3,data=df)
##r方提升，但是再继续增加项数已经难以正常拟合
##尝试用垂直正交的方法来拟合
summary(lm(y~poly(x,degree=14),data=df))
##比较不同阶次的模型
poly.fit<- lm(y~poly(x,1),data=df)
df<- transform(df,prey=predict(poly.fit))
ggplot(df,aes(x=x,y=prey))+geom_point()+geom_line()

poly.fit<- lm(y~poly(x,3),data=df)
df<- transform(df,prey=predict(poly.fit))
ggplot(df,aes(x=x,y=prey))+geom_point()+geom_line()  #3次很接近正玄波了

poly.fit<- lm(y~poly(x,5),data=df)
df<- transform(df,prey=predict(poly.fit))
ggplot(df,aes(x=x,y=prey))+geom_point()+geom_line() #5次跟3次差不多

poly.fit<- lm(y~poly(x,15),data=df)
df<- transform(df,prey=predict(poly.fit))
ggplot(df,aes(x=x,y=prey))+geom_point()+geom_line() #15次前后有些歪斜

poly.fit<- lm(y~poly(x,25),data=df)
df<- transform(df,prey=predict(poly.fit))
ggplot(df,aes(x=x,y=prey))+geom_point()+geom_line()  ##25次已经彻底变形了
##出现过拟合的问题，下面探究如何避免过拟合  使用交叉验证（cross-validation） 与正则化
##交叉验证，就是常用的，保留一部分数据作为验证。。
#再创建一遍正弦数据
set.seed(1)
x<- seq(0,1,by=0.01)
y<- sin(2*pi*x)+rnorm(length(x),0,0.1)

n<- length(x)
indices<-sort(sample(1:n,round(0.5*n)))

training.x<- x[indices]
training.y<- y[indices]

test.x<- x[-indices]
test.y<- y[-indices]

training.df<- data.frame(x=training.x,y=training.y)
test.df<- data.frame(x=test.x,y=test.y)

#写一个均方误差的函数来度量拟合效果
rmse<- function(y,h) return(sqrt(mean((y-h)^2)))

performance<- data.frame()
for(d in 1:12){
   poly.fit<- lm(y~poly(x,degree=d),data=training.df)
   performance<- rbind(performance,data.frame(degree=d,data=
'training',rmse=rmse(training.y,predict(poly.fit))))
   performance<- rbind(performance,data.frame(degree=d,data='test',
rmse=rmse(test.y,predict(poly.fit,newdata=test.df))))
}

ggplot(performance,aes(x=degree,y=rmse,linetype=data))+geom_point()+geom_line()

###中间的训练集和测试集最接近，次数过低，没有成功拟合，过高则差距很大
#####
#####交叉验证方法结束
####开始正则化方法
#拟合越多次的模型越复杂,特征项越多越复杂，权重越大越复杂

lm.fit<- lm(y~x)
l2.model.complexity<- sum(coef(lm.fit)^2)   #L2正则化
l1.model.complexity<- sum(abs(lm.fit))) #L1正则化
###
####使用glmnet包
install.packages('glmnet')
library(glmnet)
#https://www.cnblogs.com/nxld/p/6170690.html R逻辑回归
set.seed(1)
x<- seq(0,1,by=0.01)
y<- sin(2*pi*x)+ rnorm(length(x),0,0.1)
x<- matrix(x) #先把x转为矩阵
glmnet(x,y)


###案例尝试
set.seed(1)
x<- seq(0,1,0.01)
y<- sin(2*pi*x)+rnorm(length(x),0,.1)

n<- length(x)
indices<- sort(sample(1:n,round(0.5*n)))

training.x<- x[indices]
training.y<-y[indices]
test.x<- x[-indices]
test.y<- y[-indices]

df<- data.frame(x=x,y=y)

training.df<- data.frame(x=training.x,y=training.y)
test.df<- data.frame(x=test.x,y=test.y)

rmse<- function(y,h){
  return(sqrt(mean((y-h)^2)))
}

library(glmnet)
glmnet.fit<- with(training.df,glmnet(poly(x,degree=10),y))
lambdas<- glmnet.fit$lambda
performance<-data.frame()
for(lambda in lambdas){
  performance<- rbind(performance,data.frame(Lambda=lambda,
rmse=rmse(test.y,with(test.df,predict(glmnet.fit,poly(x,degree=10),s=lambda)))))
}

ggplot(performance,aes(x=Lambda,y=rmse))+geom_point()+geom_line()

###lambda 等于0.05时，模型效果最好
best.lambda<- with(performance,Lambda[which(rmse==min(rmse))])
glmnet.fit2<- with(df,glmnet(poly(x,degree=10),y))

coef(glmnet.fit2,s=best.lambda)
###只用了3个特征


######文本回归案例
ranks<- read.csv('C:/Users/fzx/Documents/ML_for_Hackers-master/06-Regularization/data/oreilly.csv',stringsAsFactors=F)
###oreilly公司所有封底的评论
library(tm)
documents <- data.frame(Text = ranks$Long.Desc.)
row.names(documents) <- 1:nrow(documents)
###开始组建语料库（文集）
##参考https://blog.csdn.net/qq_22194911/article/details/71305586
#https://www.cnblogs.com/gyjerry/p/5578638.html
corpus<- Corpus(DataframeSource(documents2))
corpus<- Corpus(VectorSource(documents))
corpus<- tm_map(corpus,content_transformer(tolower)) ##转小写
corpus<- tm_map(corpus,content_transformer(stripWhitespace))#删多余空格
corpus<- tm_map(corpus,removeWords,stopwords('english'))##删英文常用停用词
dtm<- DocumentTermMatrix(corpus)

x<-as.matrix(dtm)
y<- rev(1:100)

set.seed(1)
library(glmnet)
performance<- data.frame()

for(lambda in c(.1,.25,.5,1,2,5)){
 for(i in 1:50){
   indices<- sample(1:100,80)
   training.x<- x[indices,]
   training.y<- y[indices]
   
   test.x<- x[-indices,]
   test.y<- y[-indices]
  
   glm.fit<- glmnet(training.x,training.y)
   predicted.y<- predict(glm.fit,test.x,s=lambda)
   rmse<- sqrt(mean((predicted.y-test.y)^2))
   
   performance<- rbind(performance,data.frame(Lambda=lambda,Iteration=i,rmse=rmse))
  }
} ##电脑跑不动，就不跑了
##
ggplot(performance,aes(x=Lambda,y=rmse))+
stat_summary(fun.data='mean_cl_boot',geom='errorbar')+
stat_summray(fun.data='mean_cl_boot',geom='point')

###
#####逻辑回归
y<-rep(c(1,0),each=50)
regularized.fit<- glmnet(x,y,family='binomial')
##线性回归假定误差服从的是高斯分布，逻辑回归假定误差服从二项分布
predict(regularized.fit,newx=x,s=.001)
###把逻辑回归的原始预测转换成概率值，可以用boot包的inv.logit函数
library(boot)
inv.logit(predict(regularized.fit,newx=x,s=0.001))

##用逻辑回归预测书能否进入畅销排行榜前50 ，主要还是在对结果的转换上
set.seed(1)
performance<- data.frame()

for(i in 1:250)
{
  indices<- sample(1:100,80)
  training.x<- x[indices,]
  training.y<- y[indices]
  test.x<- x[-indices,]
  test.y<- y[-indices]

  for (lambda in c(.0001,.001,.0025,0.005,.01,.025,0.5,.1))
  {
    glm.fit<- glmnet(training.x,training.y,family='binomial')
    predicted.y<- ifelse(predict(glm.fit,test.x,s=lambda)>0,1,0)
    error.rate<- mean(predicted.y!=test.y)

    performance<- rbind(performance,data.frame(Lambda=lambda,iteration=i,errorate=error.rate))
  }
}


####

######两个逻辑回归的案例
#用R画出sigmoid函数
x<- seq(-3,3,0.01)
y<- 1/(1+exp(-x))
gdf<- data.frame(x=x,y=y)
ggplot(gdf,aes(x=x,y=y))+geom_line(col='blue')+geom_vline(xintercept=c(0),col='red')+geom_hline(yintercept=c(0,1),lty=2)
####使用莺尾花数据集
index<- which(iris$Species=='setosa')
ir<- iris[-index,]
levels(ir$Species)[1]<-''
split<- sample(100,100*(2/3))
ir_train<- ir[split,]  #训练集

ir_test<- ir[-split,] #测试集

fit<- glm(Species~.,family=binomial(link='logit'),data=ir_train)

summary(fit)

real<- ir_test$Species
predict<- predict(fit,type='response',newdata=ir_test)
data.frame(real,predict)
res<- data.frame(real,predict=ifelse(predict>0.5,'virginca','versicorlor'))
plot(res)

#####泰坦尼克号案例
titanic<- read.csv('titanic3.csv',head=T,na.strings=c(''))

set.seed(1)
training<- titanic[sort(sample(nrow(titanic),0.8*nrow(titanic),replace=F)),]
set.seed(1)
test=titanic[-sample(nrow(titanic),0.8*nrow(titanic),replace=F),]
#分割数据集和测试集
###清洗数据
library(plyr)
sapply(training,function(x) sum(is.na(x)))##统计缺失值的数量

sapply(test,function(x) sum(is.na(x)))
##
trainingdata<- subset(training,select=c(1,2,4,5,6,7,9,11))
testdata<- subset(test,select=c(1,2,4,5,6,7,9,11))##选出有用的列

trainingdata$age[is.na(trainingdata$age)]=mean(trainingdata$age,na.rm=T)
testdata$age[is.na(trainingdata$age)]=mean(testdata$age,na.rm=T)
##年龄取均值
trainingdata<- trainingdata[!is.na(trainingdata$fare),]
trainingdata<- trainingdata[!is.na(trainingdata$embarked),]
##直接删掉部分缺失行
###建立模型
logisticmodel<- glm(survived~.,family=binomial(link='logit'),data=trainingdata)
summary(logisticmodel)
##
#####预测
fitted<- predict(logisticmodel,newdata=subset(testdata,select=c(1,3,4,5,6,7,8)),type='response')
fittedsults<- ifelse(fitted>0.5,1,0)
error<- mean(fittedsults !=testdata$survived)
print(paste('accuracy',1-error))

##生成混淆矩阵
ct<-table((fittedsults),testdata$survived)
ct
plot(ct,main='confusion matrix',xlab='predicted survived',ylab='ture survived')

###绘制ROC 计算AUC
Library(ROCR)
p = predict(logisticmodel,newdata = subset(testdata,select=c(1,3,4,5,6,7,8)),type='response')
pr = prediction(p, testdata$survived)
prf = performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)

